{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6bb1597",
   "metadata": {},
   "source": [
    "# Silence Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f062a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75acd528",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee889858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.serialization import add_safe_globals\n",
    "\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "from agent import Agent\n",
    "from wrappers import apply_wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c5724a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_safe_globals([Agent])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1b8908",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41e394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY = False\n",
    "NUM_OF_EPISODES = 50_000\n",
    "CKPT_SAVE_INTERVAL = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9af0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD = True\n",
    "PATH = 'models/model_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "023a3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config if LOAD = False\n",
    "if not LOAD:\n",
    "    # Environment Configuration\n",
    "    TRAIN_LEVELS = ['SuperMarioBros-1-1-v0']\n",
    "    TEST_LEVELS = ['SuperMarioBros-1-1-v0']\n",
    "    NUM_EVAL_EPISODES = 1\n",
    "    SKIP_FRAME = 4\n",
    "    RESIZE = 84\n",
    "    FRAME_STACK = 4\n",
    "\n",
    "    # Hyperparameter Configuration\n",
    "    LR = 0.00025\n",
    "    GAMMA = 0.9\n",
    "    EPSILON = 1.0\n",
    "    EPS_DECAY = 0.99999975\n",
    "    EPS_MIN = 0.1\n",
    "    REPLAY_BUFFER_CAPACITY = 100_000\n",
    "    BATCH_SIZE = 32\n",
    "    SYNC_NETWORK_RATE = 10_000\n",
    "\n",
    "    # Network Architecture Configuration\n",
    "    conv_layers = nn.Sequential(\n",
    "        nn.Conv2d(FRAME_STACK, 32, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    o = conv_layers(torch.zeros(1, FRAME_STACK, RESIZE, RESIZE))\n",
    "    conv_out_size = int(np.prod(o.size()))\n",
    "\n",
    "    network = nn.Sequential(\n",
    "        conv_layers,\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(conv_out_size, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, len(RIGHT_ONLY))\n",
    "    )\n",
    "\n",
    "    # Create Agent\n",
    "    agent = Agent(\n",
    "        network,\n",
    "        len(RIGHT_ONLY),\n",
    "        LR,\n",
    "        GAMMA,\n",
    "        EPSILON,\n",
    "        EPS_DECAY,\n",
    "        EPS_MIN,\n",
    "        REPLAY_BUFFER_CAPACITY,\n",
    "        BATCH_SIZE,\n",
    "        SYNC_NETWORK_RATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f842ef63",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89ef203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths and load specified model if LOAD = TRUE\n",
    "if LOAD:\n",
    "    CKPT_PATH = os.path.join(PATH, \"checkpoint.pt\")\n",
    "    TEST_CSV_PATH = os.path.join(PATH, \"test.csv\")\n",
    "\n",
    "    checkpoint = torch.load(CKPT_PATH, weights_only=False)\n",
    "\n",
    "    TRAIN_LEVELS = checkpoint['train_levels']\n",
    "    TEST_LEVELS = checkpoint['test_levels']\n",
    "    NUM_EVAL_EPISODES = checkpoint['num_eval_episodes']\n",
    "    SKIP_FRAME = checkpoint['skip_frame']\n",
    "    RESIZE = checkpoint['resize']\n",
    "    FRAME_STACK = checkpoint['frame_stack']\n",
    "\n",
    "    agent = checkpoint['agent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23b3365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths if LOAD = False\n",
    "if not LOAD:\n",
    "    base = \"models\"\n",
    "    os.makedirs(base, exist_ok=True)\n",
    "\n",
    "    existing = [d for d in os.listdir(base) if d.startswith(\"model_v\")]\n",
    "    nums = [int(d.replace(\"model_v\", \"\")) for d in existing if d.replace(\"model_v\", \"\").isdigit()]\n",
    "    next_version = max(nums) + 1 if nums else 1\n",
    "\n",
    "    PATH = os.path.join(base, f\"model_v{next_version}\")\n",
    "    os.makedirs(PATH, exist_ok=True)\n",
    "    \n",
    "    CKPT_PATH = os.path.join(PATH, \"checkpoint.pt\")\n",
    "    TEST_CSV_PATH = os.path.join(PATH, \"test.csv\")\n",
    "\n",
    "    with open(TEST_CSV_PATH, \"w\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow([\"episode\", \"level\", \"reward\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf23a14",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d90def47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_episode(level):\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(NUM_EVAL_EPISODES):\n",
    "        env = gym_super_mario_bros.make(level, render_mode='rgb', apply_api_compatibility=True)\n",
    "        env = JoypadSpace(env, RIGHT_ONLY)\n",
    "        env = apply_wrappers(env, SKIP_FRAME, RESIZE, FRAME_STACK)\n",
    "\n",
    "        try:\n",
    "            state, _ = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                action = agent.choose_action_test(state)\n",
    "                state, reward, done, truncated, info = env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "            rewards.append(total_reward)\n",
    "\n",
    "        finally:\n",
    "            env.close()\n",
    "\n",
    "    return sum(rewards) / len(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fc91a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_episode(level):\n",
    "    env = gym_super_mario_bros.make(level, render_mode='human' if DISPLAY else 'rgb', apply_api_compatibility=True)\n",
    "    env = JoypadSpace(env, RIGHT_ONLY)\n",
    "    env = apply_wrappers(env, SKIP_FRAME, RESIZE, FRAME_STACK)\n",
    "\n",
    "    try:\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            new_state, reward, done, truncated, info  = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            agent.store_in_memory(state, action, reward, new_state, done)\n",
    "            agent.learn()\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "        return total_reward\n",
    "    \n",
    "    finally:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf6fd3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Episode Number: 1\n",
      "Total Episode Number 2876\n",
      "Learn step counter: 810968\n",
      "Total reward: 243.0\n",
      "Epsilon: 0.8164888476663912\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 2\n",
      "Total Episode Number 2877\n",
      "Learn step counter: 811078\n",
      "Total reward: 745.0\n",
      "Epsilon: 0.8164663945290026\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 3\n",
      "Total Episode Number 2878\n",
      "Learn step counter: 811669\n",
      "Total reward: 688.0\n",
      "Epsilon: 0.8163457705154344\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 4\n",
      "Total Episode Number 2879\n",
      "Learn step counter: 811821\n",
      "Total reward: 637.0\n",
      "Epsilon: 0.8163147499616674\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 5\n",
      "Total Episode Number 2880\n",
      "Learn step counter: 811902\n",
      "Total reward: 632.0\n",
      "Epsilon: 0.816298219753281\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 6\n",
      "Total Episode Number 2881\n",
      "Learn step counter: 812149\n",
      "Total reward: 716.0\n",
      "Epsilon: 0.8162478148881699\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 7\n",
      "Total Episode Number 2882\n",
      "Learn step counter: 813210\n",
      "Total reward: 1163.0\n",
      "Epsilon: 0.8160313338402473\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 8\n",
      "Total Episode Number 2883\n",
      "Learn step counter: 813361\n",
      "Total reward: 637.0\n",
      "Epsilon: 0.8160005292349807\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 9\n",
      "Total Episode Number 2884\n",
      "Learn step counter: 813397\n",
      "Total reward: 238.0\n",
      "Epsilon: 0.8159931852623464\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 10\n",
      "Total Episode Number 2885\n",
      "Learn step counter: 813845\n",
      "Total reward: 979.0\n",
      "Epsilon: 0.8159017991318791\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 11\n",
      "Total Episode Number 2886\n",
      "Learn step counter: 814019\n",
      "Total reward: 777.0\n",
      "Epsilon: 0.8158663081711093\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 12\n",
      "Total Episode Number 2887\n",
      "Learn step counter: 814052\n",
      "Total reward: 242.0\n",
      "Epsilon: 0.8158595773009895\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 13\n",
      "Total Episode Number 2888\n",
      "Learn step counter: 814761\n",
      "Total reward: 930.0\n",
      "Epsilon: 0.815714978988222\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 14\n",
      "Total Episode Number 2889\n",
      "Learn step counter: 814853\n",
      "Total reward: 609.0\n",
      "Epsilon: 0.8156962177571123\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 15\n",
      "Total Episode Number 2890\n",
      "Learn step counter: 815415\n",
      "Total reward: 731.0\n",
      "Epsilon: 0.8155816204748249\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 16\n",
      "Total Episode Number 2891\n",
      "Learn step counter: 815507\n",
      "Total reward: 603.0\n",
      "Epsilon: 0.8155628623109265\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 17\n",
      "Total Episode Number 2892\n",
      "Learn step counter: 815647\n",
      "Total reward: 640.0\n",
      "Epsilon: 0.8155343181067002\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 18\n",
      "Total Episode Number 2893\n",
      "Learn step counter: 816566\n",
      "Total reward: 1197.0\n",
      "Epsilon: 0.815346970596039\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 19\n",
      "Total Episode Number 2894\n",
      "Learn step counter: 818412\n",
      "Total reward: 699.0\n",
      "Epsilon: 0.8149707747359027\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 20\n",
      "Total Episode Number 2895\n",
      "Learn step counter: 818761\n",
      "Total reward: 774.0\n",
      "Epsilon: 0.8148996716288275\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 21\n",
      "Total Episode Number 2896\n",
      "Learn step counter: 818796\n",
      "Total reward: 246.0\n",
      "Epsilon: 0.8148925412870035\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 22\n",
      "Total Episode Number 2897\n",
      "Learn step counter: 819007\n",
      "Total reward: 1141.0\n",
      "Epsilon: 0.8148495568337967\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 23\n",
      "Total Episode Number 2898\n",
      "Learn step counter: 819146\n",
      "Total reward: 741.0\n",
      "Epsilon: 0.8148212413001384\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 24\n",
      "Total Episode Number 2899\n",
      "Learn step counter: 819430\n",
      "Total reward: 1410.0\n",
      "Epsilon: 0.8147633910384754\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 25\n",
      "Total Episode Number 2900\n",
      "Learn step counter: 820064\n",
      "Total reward: 944.0\n",
      "Epsilon: 0.8146342612586416\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 26\n",
      "Total Episode Number 2901\n",
      "Learn step counter: 820726\n",
      "Total reward: 711.0\n",
      "Epsilon: 0.8144994504274374\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 27\n",
      "Total Episode Number 2902\n",
      "Learn step counter: 821139\n",
      "Total reward: 1277.0\n",
      "Epsilon: 0.8144153576900206\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 28\n",
      "Total Episode Number 2903\n",
      "Learn step counter: 821527\n",
      "Total reward: 1391.0\n",
      "Epsilon: 0.814336363221733\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 29\n",
      "Total Episode Number 2904\n",
      "Learn step counter: 822089\n",
      "Total reward: 1249.0\n",
      "Epsilon: 0.8142219569856094\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 30\n",
      "Total Episode Number 2905\n",
      "Learn step counter: 822743\n",
      "Total reward: 949.0\n",
      "Epsilon: 0.8140888425613846\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 31\n",
      "Total Episode Number 2906\n",
      "Learn step counter: 822783\n",
      "Total reward: 227.0\n",
      "Epsilon: 0.8140807017126446\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 32\n",
      "Total Episode Number 2907\n",
      "Learn step counter: 822892\n",
      "Total reward: 603.0\n",
      "Epsilon: 0.814058518312997\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 33\n",
      "Total Episode Number 2908\n",
      "Learn step counter: 822974\n",
      "Total reward: 612.0\n",
      "Epsilon: 0.8140418302823362\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 34\n",
      "Total Episode Number 2909\n",
      "Learn step counter: 823081\n",
      "Total reward: 593.0\n",
      "Epsilon: 0.8140200549518976\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 35\n",
      "Total Episode Number 2910\n",
      "Learn step counter: 823158\n",
      "Total reward: 652.0\n",
      "Epsilon: 0.8140043852147003\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 36\n",
      "Total Episode Number 2911\n",
      "Learn step counter: 823192\n",
      "Total reward: 248.0\n",
      "Epsilon: 0.813997466205966\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 37\n",
      "Total Episode Number 2912\n",
      "Learn step counter: 823221\n",
      "Total reward: 250.0\n",
      "Epsilon: 0.8139915647449902\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 38\n",
      "Total Episode Number 2913\n",
      "Learn step counter: 824082\n",
      "Total reward: 671.0\n",
      "Epsilon: 0.8138163718945628\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 39\n",
      "Total Episode Number 2914\n",
      "Learn step counter: 824170\n",
      "Total reward: 619.0\n",
      "Epsilon: 0.8137984681290827\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 40\n",
      "Total Episode Number 2915\n",
      "Learn step counter: 824204\n",
      "Total reward: 247.0\n",
      "Epsilon: 0.8137915508706363\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 41\n",
      "Total Episode Number 2916\n",
      "Learn step counter: 824280\n",
      "Total reward: 613.0\n",
      "Epsilon: 0.8137760889761233\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 42\n",
      "Total Episode Number 2917\n",
      "Learn step counter: 824432\n",
      "Total reward: 813.0\n",
      "Epsilon: 0.8137451660684115\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 43\n",
      "Total Episode Number 2918\n",
      "Learn step counter: 824588\n",
      "Total reward: 636.0\n",
      "Epsilon: 0.8137134306218088\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 44\n",
      "Total Episode Number 2919\n",
      "Learn step counter: 824623\n",
      "Total reward: 251.0\n",
      "Epsilon: 0.8137063106595498\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 45\n",
      "Total Episode Number 2920\n",
      "Learn step counter: 824695\n",
      "Total reward: 623.0\n",
      "Epsilon: 0.8136916640759447\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 46\n",
      "Total Episode Number 2921\n",
      "Learn step counter: 824770\n",
      "Total reward: 608.0\n",
      "Epsilon: 0.813676407498365\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 47\n",
      "Total Episode Number 2922\n",
      "Learn step counter: 824807\n",
      "Total reward: 243.0\n",
      "Epsilon: 0.8136688810254638\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 48\n",
      "Total Episode Number 2923\n",
      "Learn step counter: 825074\n",
      "Total reward: 1297.0\n",
      "Epsilon: 0.8136145704334948\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 49\n",
      "Total Episode Number 2924\n",
      "Learn step counter: 825970\n",
      "Total reward: 888.0\n",
      "Epsilon: 0.8134323411573537\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 50\n",
      "Total Episode Number 2925\n",
      "Learn step counter: 826252\n",
      "Total reward: 787.0\n",
      "Epsilon: 0.8133749961915597\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 51\n",
      "Total Episode Number 2926\n",
      "Learn step counter: 826902\n",
      "Total reward: 937.0\n",
      "Epsilon: 0.8132428334766516\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 52\n",
      "Total Episode Number 2927\n",
      "Learn step counter: 826976\n",
      "Total reward: 615.0\n",
      "Epsilon: 0.8132277886215148\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 53\n",
      "Total Episode Number 2928\n",
      "Learn step counter: 827144\n",
      "Total reward: 756.0\n",
      "Epsilon: 0.8131936337673754\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 54\n",
      "Total Episode Number 2929\n",
      "Learn step counter: 827178\n",
      "Total reward: 243.0\n",
      "Epsilon: 0.81318672165\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 55\n",
      "Total Episode Number 2930\n",
      "Learn step counter: 827206\n",
      "Total reward: 251.0\n",
      "Epsilon: 0.8131810293621592\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 56\n",
      "Total Episode Number 2931\n",
      "Learn step counter: 827942\n",
      "Total reward: 696.0\n",
      "Epsilon: 0.8130314177987201\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 57\n",
      "Total Episode Number 2932\n",
      "Learn step counter: 827973\n",
      "Total reward: 243.0\n",
      "Epsilon: 0.8130251168288599\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 58\n",
      "Total Episode Number 2933\n",
      "Learn step counter: 828007\n",
      "Total reward: 251.0\n",
      "Epsilon: 0.8130182061438725\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 59\n",
      "Total Episode Number 2934\n",
      "Learn step counter: 828039\n",
      "Total reward: 244.0\n",
      "Epsilon: 0.813011702023426\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 60\n",
      "Total Episode Number 2935\n",
      "Learn step counter: 828177\n",
      "Total reward: 601.0\n",
      "Epsilon: 0.8129836536000343\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 61\n",
      "Total Episode Number 2936\n",
      "Learn step counter: 828249\n",
      "Total reward: 610.0\n",
      "Epsilon: 0.8129690200241406\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 62\n",
      "Total Episode Number 2937\n",
      "Learn step counter: 828284\n",
      "Total reward: 233.0\n",
      "Epsilon: 0.8129619065754466\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 63\n",
      "Total Episode Number 2938\n",
      "Learn step counter: 828428\n",
      "Total reward: 639.0\n",
      "Epsilon: 0.8129326404699406\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 64\n",
      "Total Episode Number 2939\n",
      "Learn step counter: 828533\n",
      "Total reward: 601.0\n",
      "Epsilon: 0.8129113012655361\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 65\n",
      "Total Episode Number 2940\n",
      "Learn step counter: 828711\n",
      "Total reward: 808.0\n",
      "Epsilon: 0.8128751275129751\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 66\n",
      "Total Episode Number 2941\n",
      "Learn step counter: 828928\n",
      "Total reward: 713.0\n",
      "Epsilon: 0.812831030227939\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 67\n",
      "Total Episode Number 2942\n",
      "Learn step counter: 828960\n",
      "Total reward: 249.0\n",
      "Epsilon: 0.8128245276048939\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 68\n",
      "Total Episode Number 2943\n",
      "Learn step counter: 829324\n",
      "Total reward: 771.0\n",
      "Epsilon: 0.8127505639290243\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 69\n",
      "Total Episode Number 2944\n",
      "Learn step counter: 829414\n",
      "Total reward: 623.0\n",
      "Epsilon: 0.8127322772447736\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 70\n",
      "Total Episode Number 2945\n",
      "Learn step counter: 829539\n",
      "Total reward: 575.0\n",
      "Epsilon: 0.8127068797547693\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 71\n",
      "Total Episode Number 2946\n",
      "Learn step counter: 829615\n",
      "Total reward: 636.0\n",
      "Epsilon: 0.812691438468814\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 72\n",
      "Total Episode Number 2947\n",
      "Learn step counter: 829916\n",
      "Total reward: 1022.0\n",
      "Epsilon: 0.8126302857313183\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 73\n",
      "Total Episode Number 2948\n",
      "Learn step counter: 830014\n",
      "Total reward: 621.0\n",
      "Epsilon: 0.8126103765307153\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 74\n",
      "Total Episode Number 2949\n",
      "Learn step counter: 831002\n",
      "Total reward: 559.0\n",
      "Epsilon: 0.8124096865288336\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 75\n",
      "Total Episode Number 2950\n",
      "Learn step counter: 831034\n",
      "Total reward: 243.0\n",
      "Epsilon: 0.8124031872765252\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 76\n",
      "Total Episode Number 2951\n",
      "Learn step counter: 831065\n",
      "Total reward: 240.0\n",
      "Epsilon: 0.8123968911754332\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 77\n",
      "Total Episode Number 2952\n",
      "Learn step counter: 831220\n",
      "Total reward: 636.0\n",
      "Epsilon: 0.8123654114018856\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 78\n",
      "Total Episode Number 2953\n",
      "Learn step counter: 831529\n",
      "Total reward: 1313.0\n",
      "Epsilon: 0.8123026585898604\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 79\n",
      "Total Episode Number 2954\n",
      "Learn step counter: 831603\n",
      "Total reward: 610.0\n",
      "Epsilon: 0.8122876311278003\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 80\n",
      "Total Episode Number 2955\n",
      "Learn step counter: 831633\n",
      "Total reward: 250.0\n",
      "Epsilon: 0.81228153899265\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 81\n",
      "Total Episode Number 2956\n",
      "Learn step counter: 832512\n",
      "Total reward: 2240.0\n",
      "Epsilon: 0.8121030597132501\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 82\n",
      "Total Episode Number 2957\n",
      "Learn step counter: 832547\n",
      "Total reward: 237.0\n",
      "Epsilon: 0.8120959538416767\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 83\n",
      "Total Episode Number 2958\n",
      "Learn step counter: 833094\n",
      "Total reward: 734.0\n",
      "Epsilon: 0.811984907299073\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 84\n",
      "Total Episode Number 2959\n",
      "Learn step counter: 833204\n",
      "Total reward: 606.0\n",
      "Epsilon: 0.811962578018357\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 85\n",
      "Total Episode Number 2960\n",
      "Learn step counter: 833370\n",
      "Total reward: 810.0\n",
      "Epsilon: 0.8119288822663445\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 86\n",
      "Total Episode Number 2961\n",
      "Learn step counter: 833575\n",
      "Total reward: 1336.0\n",
      "Epsilon: 0.8118872719721941\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 87\n",
      "Total Episode Number 2962\n",
      "Learn step counter: 834055\n",
      "Total reward: 1257.0\n",
      "Epsilon: 0.8117898513327232\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 88\n",
      "Total Episode Number 2963\n",
      "Learn step counter: 834206\n",
      "Total reward: 637.0\n",
      "Epsilon: 0.8117592068404196\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 89\n",
      "Total Episode Number 2964\n",
      "Learn step counter: 834358\n",
      "Total reward: 761.0\n",
      "Epsilon: 0.8117283605727827\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 90\n",
      "Total Episode Number 2965\n",
      "Learn step counter: 834514\n",
      "Total reward: 812.0\n",
      "Epsilon: 0.8116967037800704\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 91\n",
      "Total Episode Number 2966\n",
      "Learn step counter: 834554\n",
      "Total reward: 233.0\n",
      "Epsilon: 0.8116885868526015\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 92\n",
      "Total Episode Number 2967\n",
      "Learn step counter: 834642\n",
      "Total reward: 615.0\n",
      "Epsilon: 0.8116707298978835\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 93\n",
      "Total Episode Number 2968\n",
      "Learn step counter: 834732\n",
      "Total reward: 622.0\n",
      "Epsilon: 0.811652467509628\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 94\n",
      "Total Episode Number 2969\n",
      "Learn step counter: 834763\n",
      "Total reward: 248.0\n",
      "Epsilon: 0.8116461772265925\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 95\n",
      "Total Episode Number 2970\n",
      "Learn step counter: 834868\n",
      "Total reward: 611.0\n",
      "Epsilon: 0.8116248717914091\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 96\n",
      "Total Episode Number 2971\n",
      "Learn step counter: 835018\n",
      "Total reward: 778.0\n",
      "Epsilon: 0.8115944364255749\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 97\n",
      "Total Episode Number 2972\n",
      "Learn step counter: 835053\n",
      "Total reward: 247.0\n",
      "Epsilon: 0.8115873350044364\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 98\n",
      "Total Episode Number 2973\n",
      "Learn step counter: 835096\n",
      "Total reward: 223.0\n",
      "Epsilon: 0.8115786104863877\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 99\n",
      "Total Episode Number 2974\n",
      "Learn step counter: 835260\n",
      "Total reward: 635.0\n",
      "Epsilon: 0.8115453364413165\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 100\n",
      "Total Episode Number 2975\n",
      "Learn step counter: 835295\n",
      "Total reward: 249.0\n",
      "Epsilon: 0.8115382354498012\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 101\n",
      "Total Episode Number 2976\n",
      "Learn step counter: 835436\n",
      "Total reward: 639.0\n",
      "Epsilon: 0.8115096292276093\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 102\n",
      "Total Episode Number 2977\n",
      "Learn step counter: 835509\n",
      "Total reward: 618.0\n",
      "Epsilon: 0.8114948193101634\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 103\n",
      "Total Episode Number 2978\n",
      "Learn step counter: 835741\n",
      "Total reward: 693.0\n",
      "Epsilon: 0.8114477539696618\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 104\n",
      "Total Episode Number 2979\n",
      "Learn step counter: 835972\n",
      "Total reward: 1025.0\n",
      "Epsilon: 0.8114008942090949\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 105\n",
      "Total Episode Number 2980\n",
      "Learn step counter: 836512\n",
      "Total reward: 672.0\n",
      "Epsilon: 0.8112913624682289\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 106\n",
      "Total Episode Number 2981\n",
      "Learn step counter: 837116\n",
      "Total reward: 643.0\n",
      "Epsilon: 0.8111688667058292\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 107\n",
      "Total Episode Number 2982\n",
      "Learn step counter: 837793\n",
      "Total reward: 1245.0\n",
      "Epsilon: 0.8110315879755005\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 108\n",
      "Total Episode Number 2983\n",
      "Learn step counter: 837834\n",
      "Total reward: 228.0\n",
      "Epsilon: 0.8110232749432879\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 109\n",
      "Total Episode Number 2984\n",
      "Learn step counter: 838113\n",
      "Total reward: 705.0\n",
      "Epsilon: 0.8109667080355756\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 110\n",
      "Total Episode Number 2985\n",
      "Learn step counter: 838149\n",
      "Total reward: 229.0\n",
      "Epsilon: 0.810959409367134\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 111\n",
      "Total Episode Number 2986\n",
      "Learn step counter: 838189\n",
      "Total reward: 229.0\n",
      "Epsilon: 0.8109512998125733\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 112\n",
      "Total Episode Number 2987\n",
      "Learn step counter: 838414\n",
      "Total reward: 1420.0\n",
      "Epsilon: 0.8109056850791775\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 113\n",
      "Total Episode Number 2988\n",
      "Learn step counter: 838452\n",
      "Total reward: 244.0\n",
      "Epsilon: 0.8108979815107977\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 114\n",
      "Total Episode Number 2989\n",
      "Learn step counter: 839280\n",
      "Total reward: 1185.0\n",
      "Epsilon: 0.810730142979509\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 115\n",
      "Total Episode Number 2990\n",
      "Learn step counter: 839968\n",
      "Total reward: 942.0\n",
      "Epsilon: 0.8105907093691026\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 116\n",
      "Total Episode Number 2991\n",
      "Learn step counter: 840107\n",
      "Total reward: 640.0\n",
      "Epsilon: 0.8105625418278409\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 117\n",
      "Total Episode Number 2992\n",
      "Learn step counter: 840137\n",
      "Total reward: 253.0\n",
      "Epsilon: 0.8105564626308135\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 118\n",
      "Total Episode Number 2993\n",
      "Learn step counter: 840216\n",
      "Total reward: 622.0\n",
      "Epsilon: 0.8105404542967561\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 119\n",
      "Total Episode Number 2994\n",
      "Learn step counter: 840797\n",
      "Total reward: 975.0\n",
      "Epsilon: 0.8104227318308389\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 120\n",
      "Total Episode Number 2995\n",
      "Learn step counter: 841275\n",
      "Total reward: 973.0\n",
      "Epsilon: 0.8103258920885568\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 121\n",
      "Total Episode Number 2996\n",
      "Learn step counter: 841796\n",
      "Total reward: 1255.0\n",
      "Epsilon: 0.8102203540012224\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 122\n",
      "Total Episode Number 2997\n",
      "Learn step counter: 841934\n",
      "Total reward: 640.0\n",
      "Epsilon: 0.8101924018776883\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 123\n",
      "Total Episode Number 2998\n",
      "Learn step counter: 841967\n",
      "Total reward: 250.0\n",
      "Epsilon: 0.8101857178171081\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 124\n",
      "Total Episode Number 2999\n",
      "Learn step counter: 842786\n",
      "Total reward: 598.0\n",
      "Epsilon: 0.8100198492520023\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 125\n",
      "Total Episode Number 3000\n",
      "Learn step counter: 843036\n",
      "Total reward: 794.0\n",
      "Epsilon: 0.8099692245871263\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 126\n",
      "Total Episode Number 3001\n",
      "Learn step counter: 843133\n",
      "Total reward: 603.0\n",
      "Epsilon: 0.8099495830691268\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 127\n",
      "Total Episode Number 3002\n",
      "Learn step counter: 843269\n",
      "Total reward: 640.0\n",
      "Epsilon: 0.8099220452480015\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 128\n",
      "Total Episode Number 3003\n",
      "Learn step counter: 843558\n",
      "Total reward: 786.0\n",
      "Epsilon: 0.809863530486781\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 129\n",
      "Total Episode Number 3004\n",
      "Learn step counter: 844103\n",
      "Total reward: 963.0\n",
      "Epsilon: 0.8097531940837828\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 130\n",
      "Total Episode Number 3005\n",
      "Learn step counter: 844430\n",
      "Total reward: 778.0\n",
      "Epsilon: 0.8096869994576247\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 131\n",
      "Total Episode Number 3006\n",
      "Learn step counter: 844755\n",
      "Total reward: 1006.0\n",
      "Epsilon: 0.8096212150532143\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 132\n",
      "Total Episode Number 3007\n",
      "Learn step counter: 844794\n",
      "Total reward: 247.0\n",
      "Epsilon: 0.8096133212838619\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 133\n",
      "Total Episode Number 3008\n",
      "Learn step counter: 845197\n",
      "Total reward: 991.0\n",
      "Epsilon: 0.8095317568404139\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 134\n",
      "Total Episode Number 3009\n",
      "Learn step counter: 845454\n",
      "Total reward: 1559.0\n",
      "Epsilon: 0.8094797460893919\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 135\n",
      "Total Episode Number 3010\n",
      "Learn step counter: 845604\n",
      "Total reward: 637.0\n",
      "Epsilon: 0.8094493911642731\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 136\n",
      "Total Episode Number 3011\n",
      "Learn step counter: 845646\n",
      "Total reward: 238.0\n",
      "Epsilon: 0.8094408919892231\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 137\n",
      "Total Episode Number 3012\n",
      "Learn step counter: 846422\n",
      "Total reward: 688.0\n",
      "Epsilon: 0.8092838756676046\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 138\n",
      "Total Episode Number 3013\n",
      "Learn step counter: 847184\n",
      "Total reward: 1205.0\n",
      "Epsilon: 0.8091297217536261\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 139\n",
      "Total Episode Number 3014\n",
      "Learn step counter: 847276\n",
      "Total reward: 634.0\n",
      "Epsilon: 0.8091111119817104\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 140\n",
      "Total Episode Number 3015\n",
      "Learn step counter: 847444\n",
      "Total reward: 730.0\n",
      "Epsilon: 0.8090771300243806\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 141\n",
      "Total Episode Number 3016\n",
      "Learn step counter: 847474\n",
      "Total reward: 248.0\n",
      "Epsilon: 0.8090710619679014\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 142\n",
      "Total Episode Number 3017\n",
      "Learn step counter: 847516\n",
      "Total reward: 226.0\n",
      "Epsilon: 0.8090625667652874\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 143\n",
      "Total Episode Number 3018\n",
      "Learn step counter: 847626\n",
      "Total reward: 603.0\n",
      "Epsilon: 0.8090403178478413\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 144\n",
      "Total Episode Number 3019\n",
      "Learn step counter: 847882\n",
      "Total reward: 792.0\n",
      "Epsilon: 0.808988540917899\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 145\n",
      "Total Episode Number 3020\n",
      "Learn step counter: 848595\n",
      "Total reward: 1234.0\n",
      "Epsilon: 0.808844351543696\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 146\n",
      "Total Episode Number 3021\n",
      "Learn step counter: 849146\n",
      "Total reward: 1261.0\n",
      "Epsilon: 0.8087329408939132\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 147\n",
      "Total Episode Number 3022\n",
      "Learn step counter: 849249\n",
      "Total reward: 586.0\n",
      "Epsilon: 0.8087121162861972\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 148\n",
      "Total Episode Number 3023\n",
      "Learn step counter: 849484\n",
      "Total reward: 765.0\n",
      "Epsilon: 0.8086646058390531\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 149\n",
      "Total Episode Number 3024\n",
      "Learn step counter: 849588\n",
      "Total reward: 605.0\n",
      "Epsilon: 0.8086435808299964\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 150\n",
      "Total Episode Number 3025\n",
      "Learn step counter: 849685\n",
      "Total reward: 612.0\n",
      "Epsilon: 0.8086239714584719\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 151\n",
      "Total Episode Number 3026\n",
      "Learn step counter: 849841\n",
      "Total reward: 636.0\n",
      "Epsilon: 0.808592435734589\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 152\n",
      "Total Episode Number 3027\n",
      "Learn step counter: 850225\n",
      "Total reward: 1005.0\n",
      "Epsilon: 0.8085148145769205\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 153\n",
      "Total Episode Number 3028\n",
      "Learn step counter: 850584\n",
      "Total reward: 1279.0\n",
      "Epsilon: 0.8084422536194538\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 154\n",
      "Total Episode Number 3029\n",
      "Learn step counter: 850668\n",
      "Total reward: 651.0\n",
      "Epsilon: 0.8084252765082636\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 155\n",
      "Total Episode Number 3030\n",
      "Learn step counter: 850814\n",
      "Total reward: 814.0\n",
      "Epsilon: 0.8083957695204843\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 156\n",
      "Total Episode Number 3031\n",
      "Learn step counter: 850950\n",
      "Total reward: 640.0\n",
      "Epsilon: 0.8083682845281287\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 157\n",
      "Total Episode Number 3032\n",
      "Learn step counter: 851046\n",
      "Total reward: 588.0\n",
      "Epsilon: 0.8083488839196803\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 158\n",
      "Total Episode Number 3033\n",
      "Learn step counter: 851709\n",
      "Total reward: 645.0\n",
      "Epsilon: 0.8082149111786997\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 159\n",
      "Total Episode Number 3034\n",
      "Learn step counter: 851743\n",
      "Total reward: 248.0\n",
      "Epsilon: 0.8082080413802915\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 160\n",
      "Total Episode Number 3035\n",
      "Learn step counter: 852116\n",
      "Total reward: 1113.0\n",
      "Epsilon: 0.808132679484805\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 161\n",
      "Total Episode Number 3036\n",
      "Learn step counter: 852488\n",
      "Total reward: 726.0\n",
      "Epsilon: 0.8080575266308699\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 162\n",
      "Total Episode Number 3037\n",
      "Learn step counter: 853257\n",
      "Total reward: 690.0\n",
      "Epsilon: 0.8079021924839098\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 163\n",
      "Total Episode Number 3038\n",
      "Learn step counter: 853353\n",
      "Total reward: 609.0\n",
      "Epsilon: 0.8078828030615381\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 164\n",
      "Total Episode Number 3039\n",
      "Learn step counter: 853429\n",
      "Total reward: 606.0\n",
      "Epsilon: 0.807867453432181\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 165\n",
      "Total Episode Number 3040\n",
      "Learn step counter: 853520\n",
      "Total reward: 623.0\n",
      "Epsilon: 0.8078490746543748\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 166\n",
      "Total Episode Number 3041\n",
      "Learn step counter: 853791\n",
      "Total reward: 1316.0\n",
      "Epsilon: 0.8077943447267157\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 167\n",
      "Total Episode Number 3042\n",
      "Learn step counter: 853884\n",
      "Total reward: 604.0\n",
      "Epsilon: 0.8077755637241806\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 168\n",
      "Total Episode Number 3043\n",
      "Learn step counter: 853990\n",
      "Total reward: 600.0\n",
      "Epsilon: 0.8077541579526909\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 169\n",
      "Total Episode Number 3044\n",
      "Learn step counter: 854133\n",
      "Total reward: 639.0\n",
      "Epsilon: 0.8077252812541045\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 170\n",
      "Total Episode Number 3045\n",
      "Learn step counter: 854232\n",
      "Total reward: 618.0\n",
      "Epsilon: 0.8077052902982809\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 171\n",
      "Total Episode Number 3046\n",
      "Learn step counter: 854330\n",
      "Total reward: 603.0\n",
      "Epsilon: 0.807685501758603\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 172\n",
      "Total Episode Number 3047\n",
      "Learn step counter: 854406\n",
      "Total reward: 632.0\n",
      "Epsilon: 0.8076701558779354\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 173\n",
      "Total Episode Number 3048\n",
      "Learn step counter: 854668\n",
      "Total reward: 713.0\n",
      "Epsilon: 0.8076172552086207\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 174\n",
      "Total Episode Number 3049\n",
      "Learn step counter: 855149\n",
      "Total reward: 669.0\n",
      "Epsilon: 0.8075201450603946\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 175\n",
      "Total Episode Number 3050\n",
      "Learn step counter: 855258\n",
      "Total reward: 635.0\n",
      "Epsilon: 0.8074981404335023\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 176\n",
      "Total Episode Number 3051\n",
      "Learn step counter: 855353\n",
      "Total reward: 596.0\n",
      "Epsilon: 0.8074789625780052\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 177\n",
      "Total Episode Number 3052\n",
      "Learn step counter: 855426\n",
      "Total reward: 637.0\n",
      "Epsilon: 0.8074642262195638\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 178\n",
      "Total Episode Number 3053\n",
      "Learn step counter: 855456\n",
      "Total reward: 251.0\n",
      "Epsilon: 0.8074581702598193\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 179\n",
      "Total Episode Number 3054\n",
      "Learn step counter: 855774\n",
      "Total reward: 1405.0\n",
      "Epsilon: 0.8073939798788523\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 180\n",
      "Total Episode Number 3055\n",
      "Learn step counter: 855872\n",
      "Total reward: 611.0\n",
      "Epsilon: 0.807374198966187\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 181\n",
      "Total Episode Number 3056\n",
      "Learn step counter: 855949\n",
      "Total reward: 634.0\n",
      "Epsilon: 0.8073586571605019\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 182\n",
      "Total Episode Number 3057\n",
      "Learn step counter: 856036\n",
      "Total reward: 638.0\n",
      "Epsilon: 0.8073410972984745\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 183\n",
      "Total Episode Number 3058\n",
      "Learn step counter: 856384\n",
      "Total reward: 1296.0\n",
      "Epsilon: 0.8072708616695145\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 184\n",
      "Total Episode Number 3059\n",
      "Learn step counter: 856478\n",
      "Total reward: 602.0\n",
      "Epsilon: 0.8072518910247972\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 185\n",
      "Total Episode Number 3060\n",
      "Learn step counter: 856612\n",
      "Total reward: 641.0\n",
      "Epsilon: 0.807224848536028\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 186\n",
      "Total Episode Number 3061\n",
      "Learn step counter: 856933\n",
      "Total reward: 1284.0\n",
      "Epsilon: 0.8071600713330465\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 187\n",
      "Total Episode Number 3062\n",
      "Learn step counter: 856965\n",
      "Total reward: 242.0\n",
      "Epsilon: 0.8071536140774969\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 188\n",
      "Total Episode Number 3063\n",
      "Learn step counter: 857564\n",
      "Total reward: 973.0\n",
      "Epsilon: 0.8070327518584485\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 189\n",
      "Total Episode Number 3064\n",
      "Learn step counter: 857600\n",
      "Total reward: 228.0\n",
      "Epsilon: 0.8070254885954575\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 190\n",
      "Total Episode Number 3065\n",
      "Learn step counter: 858037\n",
      "Total reward: 985.0\n",
      "Epsilon: 0.8069373258657725\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 191\n",
      "Total Episode Number 3066\n",
      "Learn step counter: 858073\n",
      "Total reward: 233.0\n",
      "Epsilon: 0.8069300634616119\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 192\n",
      "Total Episode Number 3067\n",
      "Learn step counter: 858163\n",
      "Total reward: 629.0\n",
      "Epsilon: 0.8069119077371644\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 193\n",
      "Total Episode Number 3068\n",
      "Learn step counter: 858251\n",
      "Total reward: 624.0\n",
      "Epsilon: 0.8068941558682434\n",
      "Size of replay buffer: 100000\n",
      "\n",
      "Current Episode Number: 194\n",
      "Total Episode Number 3069\n",
      "Learn step counter: 858289\n",
      "Total reward: 231.0\n",
      "Epsilon: 0.8068864904092146\n",
      "Size of replay buffer: 100000\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m agent\u001b[38;5;241m.\u001b[39mepisode_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      4\u001b[0m level \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(TRAIN_LEVELS)\n\u001b[1;32m----> 5\u001b[0m train_reward \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent Episode Number:\u001b[39m\u001b[38;5;124m\"\u001b[39m, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Episode Number\u001b[39m\u001b[38;5;124m\"\u001b[39m, agent\u001b[38;5;241m.\u001b[39mepisode_counter)\n",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m, in \u001b[0;36mrun_training_episode\u001b[1;34m(level)\u001b[0m\n\u001b[0;32m     13\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     15\u001b[0m     agent\u001b[38;5;241m.\u001b[39mstore_in_memory(state, action, reward, new_state, done)\n\u001b[1;32m---> 16\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     state \u001b[38;5;241m=\u001b[39m new_state\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_reward\n",
      "File \u001b[1;32mc:\\cs_projects\\Super-Mario-Bros-RL\\agent.py:131\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Gradient descent\u001b[39;00m\n\u001b[0;32m    130\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(predicted_q_values, target_q_values)\n\u001b[1;32m--> 131\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Increment and decay epsilon\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\owenj\\miniconda3\\envs\\supermario\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\owenj\\miniconda3\\envs\\supermario\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\owenj\\miniconda3\\envs\\supermario\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(NUM_OF_EPISODES):\n",
    "    agent.episode_counter += 1\n",
    "\n",
    "    level = random.choice(TRAIN_LEVELS)\n",
    "    train_reward = run_training_episode(level)\n",
    "\n",
    "    print(\"Current Episode Number:\", i + 1)\n",
    "    print(\"Total Episode Number\", agent.episode_counter)\n",
    "    print(\"Learn step counter:\", agent.learn_step_counter)\n",
    "    print(\"Total reward:\", train_reward)\n",
    "    print(\"Epsilon:\", agent.epsilon)\n",
    "    print(\"Size of replay buffer:\", len(agent.replay_buffer))\n",
    "    print()\n",
    "\n",
    "    if (i + 1) % CKPT_SAVE_INTERVAL == 0:\n",
    "        # Save model\n",
    "        torch.save(\n",
    "            {\n",
    "                \"agent\": agent,\n",
    "                \"train_levels\": TRAIN_LEVELS,\n",
    "                \"test_levels\": TEST_LEVELS,\n",
    "                \"num_eval_episodes\": NUM_EVAL_EPISODES,\n",
    "                \"skip_frame\": SKIP_FRAME,\n",
    "                \"resize\": RESIZE,\n",
    "                \"frame_stack\": FRAME_STACK\n",
    "            },\n",
    "            CKPT_PATH\n",
    "        )\n",
    "\n",
    "        # Save testing rewards\n",
    "        with open(TEST_CSV_PATH, \"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "\n",
    "            for test_level in TEST_LEVELS:\n",
    "                reward = run_test_episode(test_level)\n",
    "                writer.writerow([agent.episode_counter, test_level, reward])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supermario",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

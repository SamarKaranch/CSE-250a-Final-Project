{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6bb1597",
   "metadata": {},
   "source": [
    "# Silence Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f062a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75acd528",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee889858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.serialization import add_safe_globals\n",
    "\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "from agent import Agent\n",
    "from wrappers import apply_wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c5724a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_safe_globals([Agent])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1b8908",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d41e394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY = False\n",
    "NUM_OF_EPISODES = 100_000\n",
    "CKPT_SAVE_INTERVAL = 50     # previously 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9af0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD = False\n",
    "PATH = 'models/model_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "023a3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config if LOAD = False\n",
    "if not LOAD:\n",
    "    # Environment Configuration\n",
    "    TRAIN_LEVELS = []\n",
    "    for world in range(1, 7):\n",
    "        for level in range(1, 5):\n",
    "            TRAIN_LEVELS.append(f\"SuperMarioBros-{world}-{level}-v0\")\n",
    "    # print(len(TRAIN_LEVELS))  # 24\n",
    "\n",
    "    TEST_LEVELS = []\n",
    "    for world in range(7, 9):\n",
    "        for level in range(1, 5):\n",
    "            TEST_LEVELS.append(f\"SuperMarioBros-{world}-{level}-v0\")\n",
    "    # print(len(TEST_LEVELS))   # 8\n",
    "\n",
    "    NUM_EVAL_EPISODES = 1\n",
    "    SKIP_FRAME = 4\n",
    "    RESIZE = 84\n",
    "    FRAME_STACK = 4\n",
    "\n",
    "    # Hyperparameter Configuration\n",
    "    LR = 0.00025\n",
    "    GAMMA = 0.9\n",
    "    EPSILON = 1.0\n",
    "    EPS_DECAY = 0.99999975\n",
    "    EPS_MIN = 0.1\n",
    "    REPLAY_BUFFER_CAPACITY = 100_000\n",
    "    BATCH_SIZE = 32\n",
    "    SYNC_NETWORK_RATE = 10_000\n",
    "\n",
    "    # Network Architecture Configuration\n",
    "    conv_layers = nn.Sequential(\n",
    "        nn.Conv2d(FRAME_STACK, 32, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    o = conv_layers(torch.zeros(1, FRAME_STACK, RESIZE, RESIZE))\n",
    "    conv_out_size = int(np.prod(o.size()))\n",
    "\n",
    "    network = nn.Sequential(\n",
    "        conv_layers,\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(conv_out_size, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, len(RIGHT_ONLY))\n",
    "    )\n",
    "\n",
    "    # Create Agent\n",
    "    agent = Agent(\n",
    "        network,\n",
    "        len(RIGHT_ONLY),\n",
    "        LR,\n",
    "        GAMMA,\n",
    "        EPSILON,\n",
    "        EPS_DECAY,\n",
    "        EPS_MIN,\n",
    "        REPLAY_BUFFER_CAPACITY,\n",
    "        BATCH_SIZE,\n",
    "        SYNC_NETWORK_RATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f842ef63",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89ef203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths and load specified model if LOAD = TRUE\n",
    "if LOAD:\n",
    "    CKPT_PATH = os.path.join(PATH, \"checkpoint.pt\")\n",
    "    TEST_CSV_PATH = os.path.join(PATH, \"test.csv\")\n",
    "    TRAIN_CSV_PATH = os.path.join(PATH, \"train_log.csv\")\n",
    "\n",
    "    checkpoint = torch.load(CKPT_PATH, weights_only=False)\n",
    "\n",
    "    TRAIN_LEVELS = checkpoint['train_levels']\n",
    "    TEST_LEVELS = checkpoint['test_levels']\n",
    "    NUM_EVAL_EPISODES = checkpoint['num_eval_episodes']\n",
    "    SKIP_FRAME = checkpoint['skip_frame']\n",
    "    RESIZE = checkpoint['resize']\n",
    "    FRAME_STACK = checkpoint['frame_stack']\n",
    "\n",
    "    agent = checkpoint['agent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23b3365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths if LOAD = False\n",
    "if not LOAD:\n",
    "    base = \"models\"\n",
    "    os.makedirs(base, exist_ok=True)\n",
    "\n",
    "    existing = [d for d in os.listdir(base) if d.startswith(\"model_v\")]\n",
    "    nums = [int(d.replace(\"model_v\", \"\")) for d in existing if d.replace(\"model_v\", \"\").isdigit()]\n",
    "    next_version = max(nums) + 1 if nums else 1\n",
    "\n",
    "    PATH = os.path.join(base, f\"model_v{next_version}\")\n",
    "    os.makedirs(PATH, exist_ok=True)\n",
    "    \n",
    "    CKPT_PATH = os.path.join(PATH, \"checkpoint.pt\")\n",
    "    TEST_CSV_PATH = os.path.join(PATH, \"test.csv\")\n",
    "    TRAIN_CSV_PATH = os.path.join(PATH, \"train_log.csv\")\n",
    "\n",
    "\n",
    "    with open(TEST_CSV_PATH, \"w\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow([\"episode\", \"level\", \"reward\"])\n",
    "\n",
    "    with open(TRAIN_CSV_PATH, \"w\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow([\"episode\", \"learn_step\", \"level\", \"reward\", \"epsilon\", \"replay_buffer_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf23a14",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d90def47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_episode(level):\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(NUM_EVAL_EPISODES):\n",
    "        env = gym_super_mario_bros.make(level, render_mode='rgb', apply_api_compatibility=True)\n",
    "        env = JoypadSpace(env, RIGHT_ONLY)\n",
    "        env = apply_wrappers(env, SKIP_FRAME, RESIZE, FRAME_STACK)\n",
    "\n",
    "        try:\n",
    "            state, _ = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                action = agent.choose_action_test(state)\n",
    "                state, reward, done, truncated, info = env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "            rewards.append(total_reward)\n",
    "\n",
    "        finally:\n",
    "            env.close()\n",
    "\n",
    "    return sum(rewards) / len(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fc91a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_episode(level):\n",
    "    env = gym_super_mario_bros.make(level, render_mode='human' if DISPLAY else 'rgb', apply_api_compatibility=True)\n",
    "    env = JoypadSpace(env, RIGHT_ONLY)\n",
    "    env = apply_wrappers(env, SKIP_FRAME, RESIZE, FRAME_STACK)\n",
    "\n",
    "    try:\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            new_state, reward, done, truncated, info  = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            agent.store_in_memory(state, action, reward, new_state, done)\n",
    "            agent.learn()\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "        return total_reward\n",
    "    \n",
    "    finally:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6fd3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Episode Number: 1\n",
      "Total Episode Number 1\n",
      "Learn step counter: 16\n",
      "Total reward: 384.0\n",
      "Epsilon: 0.9999960000074994\n",
      "Size of replay buffer: 47\n",
      "\n",
      "Current Episode Number: 2\n",
      "Total Episode Number 2\n",
      "Learn step counter: 53\n",
      "Total reward: 169.0\n",
      "Epsilon: 0.9999867500861227\n",
      "Size of replay buffer: 84\n",
      "\n",
      "Current Episode Number: 3\n",
      "Total Episode Number 3\n",
      "Learn step counter: 1548\n",
      "Total reward: 81.0\n",
      "Epsilon: 0.9996130748264299\n",
      "Size of replay buffer: 1579\n",
      "\n",
      "Current Episode Number: 4\n",
      "Total Episode Number 4\n",
      "Learn step counter: 1733\n",
      "Total reward: 342.0\n",
      "Epsilon: 0.9995668437850347\n",
      "Size of replay buffer: 1764\n",
      "\n",
      "Current Episode Number: 5\n",
      "Total Episode Number 5\n",
      "Learn step counter: 1776\n",
      "Total reward: 243.0\n",
      "Epsilon: 0.9995560984978754\n",
      "Size of replay buffer: 1807\n",
      "\n",
      "Current Episode Number: 6\n",
      "Total Episode Number 6\n",
      "Learn step counter: 1826\n",
      "Total reward: 330.0\n",
      "Epsilon: 0.9995436041231707\n",
      "Size of replay buffer: 1857\n",
      "\n",
      "Current Episode Number: 7\n",
      "Total Episode Number 7\n",
      "Learn step counter: 1883\n",
      "Total reward: 408.0\n",
      "Epsilon: 0.9995293607265141\n",
      "Size of replay buffer: 1914\n",
      "\n",
      "Current Episode Number: 8\n",
      "Total Episode Number 8\n",
      "Learn step counter: 2280\n",
      "Total reward: 300.0\n",
      "Epsilon: 0.9994301623478495\n",
      "Size of replay buffer: 2311\n",
      "\n",
      "Current Episode Number: 9\n",
      "Total Episode Number 9\n",
      "Learn step counter: 2326\n",
      "Total reward: 385.0\n",
      "Epsilon: 0.9994186689656314\n",
      "Size of replay buffer: 2357\n",
      "\n",
      "Current Episode Number: 10\n",
      "Total Episode Number 10\n",
      "Learn step counter: 2363\n",
      "Total reward: 244.0\n",
      "Epsilon: 0.9994094243845427\n",
      "Size of replay buffer: 2394\n",
      "\n",
      "Current Episode Number: 11\n",
      "Total Episode Number 11\n",
      "Learn step counter: 3009\n",
      "Total reward: 250.0\n",
      "Epsilon: 0.9992480327750314\n",
      "Size of replay buffer: 3040\n",
      "\n",
      "Current Episode Number: 12\n",
      "Total Episode Number 12\n",
      "Learn step counter: 3104\n",
      "Total reward: 553.0\n",
      "Epsilon: 0.9992243009131\n",
      "Size of replay buffer: 3135\n",
      "\n",
      "Current Episode Number: 13\n",
      "Total Episode Number 13\n",
      "Learn step counter: 3717\n",
      "Total reward: 495.0\n",
      "Epsilon: 0.9990711815028989\n",
      "Size of replay buffer: 3748\n",
      "\n",
      "Current Episode Number: 14\n",
      "Total Episode Number 14\n",
      "Learn step counter: 3764\n",
      "Total reward: 375.0\n",
      "Epsilon: 0.999059442484014\n",
      "Size of replay buffer: 3795\n",
      "\n",
      "Current Episode Number: 15\n",
      "Total Episode Number 15\n",
      "Learn step counter: 3795\n",
      "Total reward: 213.0\n",
      "Epsilon: 0.9990516998023689\n",
      "Size of replay buffer: 3826\n",
      "\n",
      "Current Episode Number: 16\n",
      "Total Episode Number 16\n",
      "Learn step counter: 3814\n",
      "Total reward: 100.0\n",
      "Epsilon: 0.9990469543174715\n",
      "Size of replay buffer: 3845\n",
      "\n",
      "Current Episode Number: 17\n",
      "Total Episode Number 17\n",
      "Learn step counter: 3852\n",
      "Total reward: 239.0\n",
      "Epsilon: 0.9990374634152996\n",
      "Size of replay buffer: 3883\n",
      "\n",
      "Current Episode Number: 18\n",
      "Total Episode Number 18\n",
      "Learn step counter: 3893\n",
      "Total reward: 244.0\n",
      "Epsilon: 0.9990272233324987\n",
      "Size of replay buffer: 3924\n",
      "\n",
      "Current Episode Number: 19\n",
      "Total Episode Number 19\n",
      "Learn step counter: 3925\n",
      "Total reward: 239.0\n",
      "Epsilon: 0.9990192311456809\n",
      "Size of replay buffer: 3956\n",
      "\n",
      "Current Episode Number: 20\n",
      "Total Episode Number 20\n",
      "Learn step counter: 4002\n",
      "Total reward: 405.0\n",
      "Epsilon: 0.9990000002081731\n",
      "Size of replay buffer: 4033\n",
      "\n",
      "Current Episode Number: 21\n",
      "Total Episode Number 21\n",
      "Learn step counter: 4067\n",
      "Total reward: 416.0\n",
      "Epsilon: 0.9989837665880369\n",
      "Size of replay buffer: 4098\n",
      "\n",
      "Current Episode Number: 22\n",
      "Total Episode Number 22\n",
      "Learn step counter: 4111\n",
      "Total reward: 317.0\n",
      "Epsilon: 0.9989727778256674\n",
      "Size of replay buffer: 4142\n",
      "\n",
      "Current Episode Number: 23\n",
      "Total Episode Number 23\n",
      "Learn step counter: 4156\n",
      "Total reward: 341.0\n",
      "Epsilon: 0.9989615394437266\n",
      "Size of replay buffer: 4187\n",
      "\n",
      "Current Episode Number: 24\n",
      "Total Episode Number 24\n",
      "Learn step counter: 4768\n",
      "Total reward: 2835.0\n",
      "Epsilon: 0.9988087100008174\n",
      "Size of replay buffer: 4799\n",
      "\n",
      "Current Episode Number: 25\n",
      "Total Episode Number 25\n",
      "Learn step counter: 4979\n",
      "Total reward: 433.0\n",
      "Epsilon: 0.9987560242243713\n",
      "Size of replay buffer: 5010\n",
      "\n",
      "Current Episode Number: 26\n",
      "Total Episode Number 26\n",
      "Learn step counter: 5016\n",
      "Total reward: 244.0\n",
      "Epsilon: 0.9987467857727189\n",
      "Size of replay buffer: 5047\n",
      "\n",
      "Current Episode Number: 27\n",
      "Total Episode Number 27\n",
      "Learn step counter: 5080\n",
      "Total reward: 385.0\n",
      "Epsilon: 0.9987308059499859\n",
      "Size of replay buffer: 5111\n",
      "\n",
      "Current Episode Number: 28\n",
      "Total Episode Number 28\n",
      "Learn step counter: 5128\n",
      "Total reward: 323.0\n",
      "Epsilon: 0.9987188212507231\n",
      "Size of replay buffer: 5159\n",
      "\n",
      "Current Episode Number: 29\n",
      "Total Episode Number 29\n",
      "Learn step counter: 5149\n",
      "Total reward: 59.0\n",
      "Epsilon: 0.9987135779900189\n",
      "Size of replay buffer: 5180\n",
      "\n",
      "Current Episode Number: 30\n",
      "Total Episode Number 30\n",
      "Learn step counter: 5301\n",
      "Total reward: 859.0\n",
      "Epsilon: 0.9986756275903684\n",
      "Size of replay buffer: 5332\n",
      "\n",
      "Current Episode Number: 31\n",
      "Total Episode Number 31\n",
      "Learn step counter: 5327\n",
      "Total reward: 115.0\n",
      "Epsilon: 0.9986691362190738\n",
      "Size of replay buffer: 5358\n",
      "\n",
      "Current Episode Number: 32\n",
      "Total Episode Number 32\n",
      "Learn step counter: 5369\n",
      "Total reward: 326.0\n",
      "Epsilon: 0.9986586502468828\n",
      "Size of replay buffer: 5400\n",
      "\n",
      "Current Episode Number: 33\n",
      "Total Episode Number 33\n",
      "Learn step counter: 5508\n",
      "Total reward: 1221.0\n",
      "Epsilon: 0.9986239474574086\n",
      "Size of replay buffer: 5539\n",
      "\n",
      "Current Episode Number: 34\n",
      "Total Episode Number 34\n",
      "Learn step counter: 5557\n",
      "Total reward: 336.0\n",
      "Epsilon: 0.9986117143874491\n",
      "Size of replay buffer: 5588\n",
      "\n",
      "Current Episode Number: 35\n",
      "Total Episode Number 35\n",
      "Learn step counter: 5900\n",
      "Total reward: 637.0\n",
      "Epsilon: 0.9985260870935473\n",
      "Size of replay buffer: 5931\n",
      "\n",
      "Current Episode Number: 36\n",
      "Total Episode Number 36\n",
      "Learn step counter: 5940\n",
      "Total reward: 221.0\n",
      "Epsilon: 0.9985161018813529\n",
      "Size of replay buffer: 5971\n",
      "\n",
      "Current Episode Number: 37\n",
      "Total Episode Number 37\n",
      "Learn step counter: 5982\n",
      "Total reward: 324.0\n",
      "Epsilon: 0.9985056175160143\n",
      "Size of replay buffer: 6013\n",
      "\n",
      "Current Episode Number: 38\n",
      "Total Episode Number 38\n",
      "Learn step counter: 6006\n",
      "Total reward: 122.0\n",
      "Epsilon: 0.9984996264995325\n",
      "Size of replay buffer: 6037\n",
      "\n",
      "Current Episode Number: 39\n",
      "Total Episode Number 39\n",
      "Learn step counter: 6039\n",
      "Total reward: 218.0\n",
      "Epsilon: 0.9984913889105632\n",
      "Size of replay buffer: 6070\n",
      "\n",
      "Current Episode Number: 40\n",
      "Total Episode Number 40\n",
      "Learn step counter: 6072\n",
      "Total reward: 140.0\n",
      "Epsilon: 0.9984831513895536\n",
      "Size of replay buffer: 6103\n",
      "\n",
      "Current Episode Number: 41\n",
      "Total Episode Number 41\n",
      "Learn step counter: 6122\n",
      "Total reward: 257.0\n",
      "Epsilon: 0.9984706704266056\n",
      "Size of replay buffer: 6153\n",
      "\n",
      "Current Episode Number: 42\n",
      "Total Episode Number 42\n",
      "Learn step counter: 6240\n",
      "Total reward: 650.0\n",
      "Epsilon: 0.9984412159725972\n",
      "Size of replay buffer: 6271\n",
      "\n",
      "Current Episode Number: 43\n",
      "Total Episode Number 43\n",
      "Learn step counter: 6776\n",
      "Total reward: 607.0\n",
      "Epsilon: 0.99830743379652\n",
      "Size of replay buffer: 6807\n",
      "\n",
      "Current Episode Number: 44\n",
      "Total Episode Number 44\n",
      "Learn step counter: 6810\n",
      "Total reward: 167.0\n",
      "Epsilon: 0.9982989482183339\n",
      "Size of replay buffer: 6841\n",
      "\n",
      "Current Episode Number: 45\n",
      "Total Episode Number 45\n",
      "Learn step counter: 7012\n",
      "Total reward: 808.0\n",
      "Epsilon: 0.9982485353880753\n",
      "Size of replay buffer: 7043\n",
      "\n",
      "Current Episode Number: 46\n",
      "Total Episode Number 46\n",
      "Learn step counter: 7238\n",
      "Total reward: 1311.0\n",
      "Epsilon: 0.9981921359320677\n",
      "Size of replay buffer: 7269\n",
      "\n",
      "Current Episode Number: 47\n",
      "Total Episode Number 47\n",
      "Learn step counter: 7290\n",
      "Total reward: 389.0\n",
      "Epsilon: 0.9981791595170236\n",
      "Size of replay buffer: 7321\n",
      "\n",
      "Current Episode Number: 48\n",
      "Total Episode Number 48\n",
      "Learn step counter: 7331\n",
      "Total reward: 240.0\n",
      "Epsilon: 0.9981689282317936\n",
      "Size of replay buffer: 7362\n",
      "\n",
      "Current Episode Number: 49\n",
      "Total Episode Number 49\n",
      "Learn step counter: 7358\n",
      "Total reward: 209.0\n",
      "Epsilon: 0.9981621906134244\n",
      "Size of replay buffer: 7389\n",
      "\n",
      "Current Episode Number: 50\n",
      "Total Episode Number 50\n",
      "Learn step counter: 7448\n",
      "Total reward: 572.0\n",
      "Epsilon: 0.9981397322139832\n",
      "Size of replay buffer: 7479\n",
      "\n",
      "Current Episode Number: 51\n",
      "Total Episode Number 51\n",
      "Learn step counter: 7717\n",
      "Total reward: 326.0\n",
      "Epsilon: 0.9980726095656165\n",
      "Size of replay buffer: 7748\n",
      "\n",
      "Current Episode Number: 52\n",
      "Total Episode Number 52\n",
      "Learn step counter: 7748\n",
      "Total reward: 170.0\n",
      "Epsilon: 0.9980648745318976\n",
      "Size of replay buffer: 7779\n",
      "\n",
      "Current Episode Number: 53\n",
      "Total Episode Number 53\n",
      "Learn step counter: 7780\n",
      "Total reward: 226.0\n",
      "Epsilon: 0.99805689004384\n",
      "Size of replay buffer: 7811\n",
      "\n",
      "Current Episode Number: 54\n",
      "Total Episode Number 54\n",
      "Learn step counter: 7835\n",
      "Total reward: 437.0\n",
      "Epsilon: 0.9980431668542317\n",
      "Size of replay buffer: 7866\n",
      "\n",
      "Current Episode Number: 55\n",
      "Total Episode Number 55\n",
      "Learn step counter: 7985\n",
      "Total reward: 1208.0\n",
      "Epsilon: 0.9980057409325312\n",
      "Size of replay buffer: 8016\n",
      "\n",
      "Current Episode Number: 56\n",
      "Total Episode Number 56\n",
      "Learn step counter: 8026\n",
      "Total reward: 316.0\n",
      "Epsilon: 0.9979955114248329\n",
      "Size of replay buffer: 8057\n",
      "\n",
      "Current Episode Number: 57\n",
      "Total Episode Number 57\n",
      "Learn step counter: 8065\n",
      "Total reward: 243.0\n",
      "Epsilon: 0.9979857810148145\n",
      "Size of replay buffer: 8096\n",
      "\n",
      "Current Episode Number: 58\n",
      "Total Episode Number 58\n",
      "Learn step counter: 8097\n",
      "Total reward: 170.0\n",
      "Epsilon: 0.9979777971595027\n",
      "Size of replay buffer: 8128\n",
      "\n",
      "Current Episode Number: 59\n",
      "Total Episode Number 59\n",
      "Learn step counter: 8145\n",
      "Total reward: 319.0\n",
      "Epsilon: 0.9979658214962923\n",
      "Size of replay buffer: 8176\n",
      "\n",
      "Current Episode Number: 60\n",
      "Total Episode Number 60\n",
      "Learn step counter: 8178\n",
      "Total reward: 250.0\n",
      "Epsilon: 0.9979575883111965\n",
      "Size of replay buffer: 8209\n",
      "\n",
      "Current Episode Number: 61\n",
      "Total Episode Number 61\n",
      "Learn step counter: 8208\n",
      "Total reward: 211.0\n",
      "Epsilon: 0.9979501036564149\n",
      "Size of replay buffer: 8239\n",
      "\n",
      "Current Episode Number: 62\n",
      "Total Episode Number 62\n",
      "Learn step counter: 8395\n",
      "Total reward: 795.0\n",
      "Epsilon: 0.9979034505737552\n",
      "Size of replay buffer: 8426\n",
      "\n",
      "Current Episode Number: 63\n",
      "Total Episode Number 63\n",
      "Learn step counter: 8698\n",
      "Total reward: 1366.0\n",
      "Epsilon: 0.9978278622408586\n",
      "Size of replay buffer: 8729\n",
      "\n",
      "Current Episode Number: 64\n",
      "Total Episode Number 64\n",
      "Learn step counter: 9568\n",
      "Total reward: 205.0\n",
      "Epsilon: 0.997610858253704\n",
      "Size of replay buffer: 9599\n",
      "\n",
      "Current Episode Number: 65\n",
      "Total Episode Number 65\n",
      "Learn step counter: 9590\n",
      "Total reward: 123.0\n",
      "Epsilon: 0.9976053714083859\n",
      "Size of replay buffer: 9621\n",
      "\n",
      "Current Episode Number: 66\n",
      "Total Episode Number 66\n",
      "Learn step counter: 11286\n",
      "Total reward: 164.0\n",
      "Epsilon: 0.9971824763380789\n",
      "Size of replay buffer: 11317\n",
      "\n",
      "Current Episode Number: 67\n",
      "Total Episode Number 67\n",
      "Learn step counter: 11315\n",
      "Total reward: 211.0\n",
      "Epsilon: 0.9971752467904279\n",
      "Size of replay buffer: 11346\n",
      "\n",
      "Current Episode Number: 68\n",
      "Total Episode Number 68\n",
      "Learn step counter: 11403\n",
      "Total reward: 571.0\n",
      "Epsilon: 0.9971533091735678\n",
      "Size of replay buffer: 11434\n",
      "\n",
      "Current Episode Number: 69\n",
      "Total Episode Number 69\n",
      "Learn step counter: 11432\n",
      "Total reward: 170.0\n",
      "Epsilon: 0.997146079837378\n",
      "Size of replay buffer: 11463\n",
      "\n",
      "Current Episode Number: 70\n",
      "Total Episode Number 70\n",
      "Learn step counter: 11482\n",
      "Total reward: 337.0\n",
      "Epsilon: 0.9971336155877221\n",
      "Size of replay buffer: 11513\n",
      "\n",
      "Current Episode Number: 71\n",
      "Total Episode Number 71\n",
      "Learn step counter: 11528\n",
      "Total reward: 430.0\n",
      "Epsilon: 0.9971221486156435\n",
      "Size of replay buffer: 11559\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(NUM_OF_EPISODES):\n",
    "    agent.episode_counter += 1\n",
    "\n",
    "    level = random.choice(TRAIN_LEVELS)\n",
    "    train_reward = run_training_episode(level)\n",
    "\n",
    "    print(\"Current Episode Number:\", i + 1)\n",
    "    print(\"Total Episode Number\", agent.episode_counter)\n",
    "    print(\"Learn step counter:\", agent.learn_step_counter)\n",
    "    print(\"Total reward:\", train_reward)\n",
    "    print(\"Epsilon:\", agent.epsilon)\n",
    "    print(\"Size of replay buffer:\", len(agent.replay_buffer))\n",
    "    print()\n",
    "\n",
    "    # Save training data at every episode\n",
    "    with open(TRAIN_CSV_PATH, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            agent.episode_counter,\n",
    "            agent.learn_step_counter,\n",
    "            level,\n",
    "            train_reward,\n",
    "            agent.epsilon,\n",
    "            len(agent.replay_buffer)\n",
    "        ])\n",
    "\n",
    "    if (i + 1) % CKPT_SAVE_INTERVAL == 0:\n",
    "        # Save model\n",
    "        torch.save(\n",
    "            {\n",
    "                \"agent\": agent,\n",
    "                \"train_levels\": TRAIN_LEVELS,\n",
    "                \"test_levels\": TEST_LEVELS,\n",
    "                \"num_eval_episodes\": NUM_EVAL_EPISODES,\n",
    "                \"skip_frame\": SKIP_FRAME,\n",
    "                \"resize\": RESIZE,\n",
    "                \"frame_stack\": FRAME_STACK\n",
    "            },\n",
    "            CKPT_PATH\n",
    "        )            \n",
    "\n",
    "        # Save testing rewards\n",
    "        with open(TEST_CSV_PATH, \"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "\n",
    "            for test_level in TEST_LEVELS:\n",
    "                reward = run_test_episode(test_level)\n",
    "                writer.writerow([agent.episode_counter, test_level, reward])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse250mario",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
